{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline"
      ],
      "metadata": {
        "id": "VRJzfH0bTvfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "\n",
        "# Import necessary modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Imputer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import pyspark.sql.types as tp\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DiabetesPredictionPipeline\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the CSV file\n",
        "my_data = spark.read.csv('/content/diabaties.csv', header=True)\n",
        "\n",
        "# Define the schema for the data\n",
        "my_schema = tp.StructType([\n",
        "    tp.StructField(name='Pregnancies', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='Glucose', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='BloodPressure', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='SkinThickness', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='Insulin', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='BMI', dataType=tp.DoubleType(), nullable=True),\n",
        "    tp.StructField(name='DiabetesPedigreeFunction', dataType=tp.DoubleType(), nullable=True),\n",
        "    tp.StructField(name='Age', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='Outcome', dataType=tp.IntegerType(), nullable=True)\n",
        "])\n",
        "\n",
        "# Read the data again with the defined schema\n",
        "my_data = spark.read.csv('/content/diabaties.csv', schema=my_schema, header=True)\n",
        "\n",
        "# Print the schema\n",
        "my_data.printSchema()\n",
        "\n",
        "# Define stages for the pipeline\n",
        "imputer = Imputer(\n",
        "    inputCols=my_data.columns,\n",
        "    outputCols=[\"{}_imputed\".format(c) for c in my_data.columns]\n",
        ").setStrategy(\"median\")\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
        "               'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='Outcome', maxIter=10)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[imputer, assembler, lr])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "xtrain, xtest = my_data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Fit the pipeline on training data\n",
        "pipeline_model = pipeline.fit(xtrain)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = pipeline_model.transform(xtest)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\")\n",
        "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqQqUtWyTu3m",
        "outputId": "ab617883-a9c2-44df-e255-dfa51f7bff6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "root\n",
            " |-- Pregnancies: integer (nullable = true)\n",
            " |-- Glucose: integer (nullable = true)\n",
            " |-- BloodPressure: integer (nullable = true)\n",
            " |-- SkinThickness: integer (nullable = true)\n",
            " |-- Insulin: integer (nullable = true)\n",
            " |-- BMI: double (nullable = true)\n",
            " |-- DiabetesPedigreeFunction: double (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Outcome: integer (nullable = true)\n",
            "\n",
            "Accuracy: 0.7991266375545851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "\n",
        "# Import necessary modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Imputer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import pyspark.sql.types as tp\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DiabetesPredictionPipeline\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the CSV file\n",
        "my_data = spark.read.csv('/content/diabetes.csv', header=True)\n",
        "\n",
        "# Define the schema for the data\n",
        "my_schema = tp.StructType([\n",
        "    tp.StructField(name='Pregnancies', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='Glucose', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='BloodPressure', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='SkinThickness', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='Insulin', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='BMI', dataType=tp.DoubleType(), nullable=True),\n",
        "    tp.StructField(name='DiabetesPedigreeFunction', dataType=tp.DoubleType(), nullable=True),\n",
        "    tp.StructField(name='Age', dataType=tp.IntegerType(), nullable=True),\n",
        "    tp.StructField(name='Outcome', dataType=tp.IntegerType(), nullable=True)\n",
        "])\n",
        "\n",
        "# Read the data again with the defined schema\n",
        "my_data = spark.read.csv('/content/diabetes.csv', schema=my_schema, header=True)\n",
        "\n",
        "# Print the schema\n",
        "my_data.printSchema()\n",
        "\n",
        "# Define stages for the pipeline\n",
        "imputer = Imputer(\n",
        "    inputCols=my_data.columns,\n",
        "    outputCols=[\"{}_imputed\".format(c) for c in my_data.columns]\n",
        ").setStrategy(\"median\")\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
        "               'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='Outcome', maxIter=10)\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(stages=[imputer, assembler, lr])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "xtrain, xtest = my_data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Fit the pipeline on training data\n",
        "pipeline_model = pipeline.fit(xtrain)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = pipeline_model.transform(xtest)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\")\n",
        "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "uz80nlJAysvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Shingles"
      ],
      "metadata": {
        "id": "cBCDz7enU6eX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGsOWM7TRs_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "212593c3-d8b1-4fe7-a9a2-37bcef00cdeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incidence Matrix:\n",
            "                                             /content/shingle1.txt  \\\n",
            "combat crime and so on\".[8]                                      0   \n",
            "of data now available are                                        0   \n",
            "sometimes loosely partly due to                                  1   \n",
            "for large enterprises is determining                             0   \n",
            "many entries (rows) offer greater                                1   \n",
            "...                                                            ...   \n",
            "estimated to reach $215.7 billion                                0   \n",
            "or insightfulness of the data.[5]                                1   \n",
            "data presents challenges in sampling,                            1   \n",
            "devices, aerial (remote sensing) equipment,                      0   \n",
            "analytics methods that extract value                             0   \n",
            "\n",
            "                                             /content/shingle2.txt  \\\n",
            "combat crime and so on\".[8]                                      1   \n",
            "of data now available are                                        1   \n",
            "sometimes loosely partly due to                                  0   \n",
            "for large enterprises is determining                             0   \n",
            "many entries (rows) offer greater                                0   \n",
            "...                                                            ...   \n",
            "estimated to reach $215.7 billion                                0   \n",
            "or insightfulness of the data.[5]                                1   \n",
            "data presents challenges in sampling,                            1   \n",
            "devices, aerial (remote sensing) equipment,                      0   \n",
            "analytics methods that extract value                             1   \n",
            "\n",
            "                                             /content/shingle3.txt  \n",
            "combat crime and so on\".[8]                                      1  \n",
            "of data now available are                                        1  \n",
            "sometimes loosely partly due to                                  0  \n",
            "for large enterprises is determining                             1  \n",
            "many entries (rows) offer greater                                0  \n",
            "...                                                            ...  \n",
            "estimated to reach $215.7 billion                                1  \n",
            "or insightfulness of the data.[5]                                0  \n",
            "data presents challenges in sampling,                            0  \n",
            "devices, aerial (remote sensing) equipment,                      1  \n",
            "analytics methods that extract value                             1  \n",
            "\n",
            "[554 rows x 3 columns]\n",
            "\n",
            "Signature Matrix:\n",
            "                       Hash1  Hash2  Hash3\n",
            "/content/shingle1.txt      0      2      7\n",
            "/content/shingle2.txt      2      4      0\n",
            "/content/shingle3.txt      2      0      0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def k_shingles(text, k):\n",
        "    words = text.split()\n",
        "    shingles = set()\n",
        "    for i in range(len(words) - k + 1):\n",
        "        shingle = ' '.join(words[i:i + k])\n",
        "        shingles.add(shingle)\n",
        "    return shingles\n",
        "\n",
        "# Define the hash functions\n",
        "def hash_function_1(x):\n",
        "    return (3 * x + 5) % 554\n",
        "\n",
        "def hash_function_2(x):\n",
        "    return (7 * x + 4) % 554\n",
        "\n",
        "def hash_function_3(x):\n",
        "    return (3 * x + 1) % 554\n",
        "\n",
        "# File paths\n",
        "file_paths = [\n",
        "    \"/content/shingle1.txt\",\n",
        "    \"/content/shingle2.txt\",\n",
        "    \"/content/shingle3.txt\"\n",
        "]\n",
        "\n",
        "# Token size\n",
        "k = 5\n",
        "\n",
        "# Dictionary to hold shingles for each file\n",
        "shingle_dict = {}\n",
        "\n",
        "# Process each file\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    shingles = k_shingles(content, k)\n",
        "    shingle_dict[file_path] = shingles\n",
        "\n",
        "# Create a set of all unique shingles across all files\n",
        "all_shingles = list(set.union(*[set(shingles) for shingles in shingle_dict.values()]))\n",
        "\n",
        "# Initialize an incidence matrix\n",
        "incidence_matrix = pd.DataFrame(0, index=all_shingles, columns=file_paths)\n",
        "\n",
        "# Populate the incidence matrix\n",
        "for file_path, shingles in shingle_dict.items():\n",
        "    for shingle in shingles:\n",
        "        incidence_matrix.at[shingle, file_path] = 1\n",
        "\n",
        "# Function to compute signatures\n",
        "def compute_signature(incidence_matrix):\n",
        "    signatures = {file_path: [] for file_path in incidence_matrix.columns}\n",
        "\n",
        "    for shingle in incidence_matrix.index:\n",
        "        # Use the index of the shingle in the all_shingles list\n",
        "        index = list(incidence_matrix.index).index(shingle)\n",
        "\n",
        "        for file_path in incidence_matrix.columns:\n",
        "            # Calculate hash values\n",
        "            hash_value = [\n",
        "                hash_function_1(index),\n",
        "                hash_function_2(index),\n",
        "                hash_function_3(index)\n",
        "            ]\n",
        "            # Append hash values if the shingle is present in the file\n",
        "            if incidence_matrix.at[shingle, file_path] == 1:\n",
        "                signatures[file_path].append(hash_value)\n",
        "\n",
        "    # Find the minimum hash value for each hash function per file\n",
        "    final_signatures = {}\n",
        "    for file_path, values in signatures.items():\n",
        "        if values:  # If there are any values\n",
        "            final_signatures[file_path] = [\n",
        "                min(value[0] for value in values),  # Min for Hash1\n",
        "                min(value[1] for value in values),  # Min for Hash2\n",
        "                min(value[2] for value in values)   # Min for Hash3\n",
        "            ]\n",
        "        else:\n",
        "            final_signatures[file_path] = [None, None, None]  # Handle case with no shingles\n",
        "\n",
        "    return pd.DataFrame(final_signatures, index=['Hash1', 'Hash2', 'Hash3']).T\n",
        "\n",
        "# Compute signatures\n",
        "signature_matrix = compute_signature(incidence_matrix)\n",
        "\n",
        "# Display the incidence matrix and signature matrix\n",
        "print(\"Incidence Matrix:\")\n",
        "print(incidence_matrix)\n",
        "\n",
        "print(\"\\nSignature Matrix:\")\n",
        "print(signature_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloom Filter"
      ],
      "metadata": {
        "id": "uUzOvNHmDkBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BloomFilter:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.bit_array = [0] * size\n",
        "\n",
        "    def hash1(self, x):\n",
        "        return (x + 1) % self.size\n",
        "\n",
        "    def hash2(self, x):\n",
        "        return (2 * x + 5) % self.size\n",
        "\n",
        "    def add(self, x):\n",
        "        index1 = self.hash1(x)\n",
        "        index2 = self.hash2(x)\n",
        "        self.bit_array[index1] = 1\n",
        "        self.bit_array[index2] = 1\n",
        "\n",
        "    def check(self, x):\n",
        "        index1 = self.hash1(x)\n",
        "        index2 = self.hash2(x)\n",
        "        return self.bit_array[index1] == 1 and self.bit_array[index2] == 1\n",
        "\n",
        "# Initialize Bloom filter\n",
        "bloom_filter = BloomFilter(13)\n",
        "\n",
        "# Add elements 8, 17, 25, 14, 20 to the Bloom filter\n",
        "elements_to_add = [8, 17, 25, 14, 20]\n",
        "for elem in elements_to_add:\n",
        "    bloom_filter.add(elem)\n",
        "\n",
        "# Check for integers 7 and 5\n",
        "check_elements = [7, 5]\n",
        "for elem in check_elements:\n",
        "    if bloom_filter.check(elem):\n",
        "        print(f\"Element {elem} may be in the set.\")\n",
        "    else:\n",
        "        print(f\"Element {elem} is definitely not in the set.\")\n"
      ],
      "metadata": {
        "id": "0vRSjDILScrN",
        "outputId": "bac284b2-6639-44e0-c99c-9ea564f981cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element 7 may be in the set.\n",
            "Element 5 may be in the set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AMS(With Given Values)"
      ],
      "metadata": {
        "id": "aeUE2ViYE4iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# AMS Algorithm Implementation\n",
        "def ams_algorithm(stream, x_values):\n",
        "    n = len(stream)\n",
        "\n",
        "    # Initialize the sum of square estimates\n",
        "    sum_squared_estimates = 0\n",
        "\n",
        "    # Perform the AMS estimate for each x_value\n",
        "    for x in x_values:\n",
        "        # Choose a random element r from the stream\n",
        "        r = stream[x - 1]  # x is 1-indexed, so we use x-1 for 0-indexed lists\n",
        "\n",
        "        # Count the number of times r appears in the stream\n",
        "        count_r = stream.count(r)\n",
        "\n",
        "        # Calculate the square of count_r and update the sum of square estimates\n",
        "        sum_squared_estimates += n * (2 * count_r - 1)\n",
        "\n",
        "    # Return the average of the square estimates\n",
        "    return sum_squared_estimates / len(x_values)\n",
        "\n",
        "# Given stream and x values\n",
        "stream = [2, 3, 7, 1, 5, 8, 5, 7, 9, 6, 4, 4, 5, 6, 5, 8, 8, 5,2, 2, 2, 1, 1, 6, 7]\n",
        "x_values = [1, 3, 5, 10]  # Values of x1, x2, x3, and x4 as 1, 3, 5, and 10\n",
        "\n",
        "# Calculate the surprise number using AMS algorithm\n",
        "surprise_number = ams_algorithm(stream, x_values)\n",
        "\n",
        "# Output the result\n",
        "print(\"The surprise number (Second Frequency Moment Estimate) is:\", surprise_number)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a67DpTSONMb_",
        "outputId": "04ac9a60-67d3-4937-fa17-fd059a5a8898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The surprise number (Second Frequency Moment Estimate) is: 162.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AMS(with Random numbers)"
      ],
      "metadata": {
        "id": "NF0S2uHqTIjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# AMS Algorithm Implementation\n",
        "def ams_algorithm(stream, x_values):\n",
        "    n = len(stream)\n",
        "\n",
        "    # Initialize the sum of square estimates\n",
        "    sum_squared_estimates = 0\n",
        "\n",
        "    # Perform the AMS estimate for each x_value\n",
        "    for x in x_values:\n",
        "        # Choose a random element r from the stream\n",
        "        r = stream[x - 1]  # x is 1-indexed, so we use x-1 for 0-indexed lists\n",
        "\n",
        "        # Count the number of times r appears in the stream\n",
        "        count_r = stream.count(r)\n",
        "\n",
        "        # Calculate the square of count_r and update the sum of square estimates\n",
        "        sum_squared_estimates += n * (2 * count_r - 1)\n",
        "\n",
        "    # Return the average of the square estimates\n",
        "    return sum_squared_estimates / len(x_values)\n",
        "\n",
        "# Generate a random stream of integers\n",
        "random_stream = [random.randint(1, 10) for _ in range(25)]\n",
        "\n",
        "# Given x values\n",
        "x_values = [1, 3, 5, 10]  # Values of x1, x2, x3, and x4 as 1, 3, 5, and 10\n",
        "\n",
        "# Calculate the surprise number using AMS algorithm on the random stream\n",
        "surprise_number = ams_algorithm(random_stream, x_values)\n",
        "\n",
        "# Output the result\n",
        "print(\"The random stream is:\", random_stream)\n",
        "print(\"The surprise number (Second Frequency Moment Estimate) is:\", surprise_number)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k05t-q70THwZ",
        "outputId": "547ad68d-67cc-4015-cf6c-cf1d4dd97d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The random stream is: [7, 10, 6, 8, 8, 6, 7, 6, 2, 3, 7, 9, 4, 8, 1, 7, 10, 8, 1, 9, 6, 5, 7, 9, 5]\n",
            "The surprise number (Second Frequency Moment Estimate) is: 150.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flajolet Martin algo"
      ],
      "metadata": {
        "id": "UIh3rzDxTYOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hash_value(x, a, b, m):\n",
        "    # Hash function: (a*x + b) mod m\n",
        "    return (a * x + b) % m\n",
        "\n",
        "def tail_length(bin_str):\n",
        "    # Find the length of trailing zeros in the binary representation of the hash value\n",
        "    return len(bin_str) - len(bin_str.rstrip('0'))\n",
        "\n",
        "def flajolet_martin(stream, a, b, m):\n",
        "    max_tail_length = 0\n",
        "    tail_lengths = []  # List to store tail lengths for each element\n",
        "\n",
        "    for element in stream:\n",
        "        # Hash the element and convert to binary\n",
        "        hash_val = hash_value(element, a, b, m)\n",
        "        bin_hash = format(hash_val, 'b').zfill(len(bin(hash_val)[2:]))  # Convert to binary and pad with zeros\n",
        "\n",
        "        # Calculate the tail length of the binary hash\n",
        "        t_len = tail_length(bin_hash)\n",
        "        tail_lengths.append((element, hash_val, bin_hash, t_len))  # Store element, hash, and tail length\n",
        "\n",
        "        # Keep track of the maximum tail length\n",
        "        max_tail_length = max(max_tail_length, t_len)\n",
        "\n",
        "    # Print the tail lengths\n",
        "    for elem, h_val, bin_h, t_len in tail_lengths:\n",
        "        print(f\"Element: {elem}, Hash: {h_val}, Binary Hash: {bin_h}, Tail Length: {t_len}\")\n",
        "\n",
        "    # Estimate the number of distinct elements\n",
        "    return 2 ** max_tail_length\n",
        "\n",
        "# Example usage\n",
        "stream = [3, 1, 4, 1, 5, 9, 2, 6, 5]\n",
        "\n",
        "# Use hash function 3x + 7 mod 32\n",
        "a1, b1, m1 = 3, 7, 32\n",
        "estimate1 = flajolet_martin(stream, a1, b1, m1)\n",
        "print(f\"\\nEstimated number of distinct elements with hash function 3x + 7 mod 32: {estimate1}\")\n",
        "\n",
        "# Use hash function 2x + 1 mod 32\n",
        "a2, b2, m2 = 2, 1, 32\n",
        "estimate2 = flajolet_martin(stream, a2, b2, m2)\n",
        "print(f\"\\nEstimated number of distinct elements with hash function 2x + 1 mod 32: {estimate2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzLQ2Cw0UJ92",
        "outputId": "9570412b-478c-41d1-d172-ab65144d2a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element: 3, Hash: 16, Binary Hash: 10000, Tail Length: 4\n",
            "Element: 1, Hash: 10, Binary Hash: 1010, Tail Length: 1\n",
            "Element: 4, Hash: 19, Binary Hash: 10011, Tail Length: 0\n",
            "Element: 1, Hash: 10, Binary Hash: 1010, Tail Length: 1\n",
            "Element: 5, Hash: 22, Binary Hash: 10110, Tail Length: 1\n",
            "Element: 9, Hash: 2, Binary Hash: 10, Tail Length: 1\n",
            "Element: 2, Hash: 13, Binary Hash: 1101, Tail Length: 0\n",
            "Element: 6, Hash: 25, Binary Hash: 11001, Tail Length: 0\n",
            "Element: 5, Hash: 22, Binary Hash: 10110, Tail Length: 1\n",
            "\n",
            "Estimated number of distinct elements with hash function 3x + 7 mod 32: 16\n",
            "Element: 3, Hash: 7, Binary Hash: 111, Tail Length: 0\n",
            "Element: 1, Hash: 3, Binary Hash: 11, Tail Length: 0\n",
            "Element: 4, Hash: 9, Binary Hash: 1001, Tail Length: 0\n",
            "Element: 1, Hash: 3, Binary Hash: 11, Tail Length: 0\n",
            "Element: 5, Hash: 11, Binary Hash: 1011, Tail Length: 0\n",
            "Element: 9, Hash: 19, Binary Hash: 10011, Tail Length: 0\n",
            "Element: 2, Hash: 5, Binary Hash: 101, Tail Length: 0\n",
            "Element: 6, Hash: 13, Binary Hash: 1101, Tail Length: 0\n",
            "Element: 5, Hash: 11, Binary Hash: 1011, Tail Length: 0\n",
            "\n",
            "Estimated number of distinct elements with hash function 2x + 1 mod 32: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bipartite\n"
      ],
      "metadata": {
        "id": "fhC7YjeRF7bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Greedy algorithm for bipartite matching\n",
        "class BipartiteMatcher:\n",
        "    def __init__(self, U, V, edges):\n",
        "        # U and V are the sets of vertices in the bipartite graph\n",
        "        # edges is a list of tuples representing edges between U and V\n",
        "        self.U = U\n",
        "        self.V = V\n",
        "        self.edges = edges\n",
        "        self.matching = set()\n",
        "        self.matched_U = set()\n",
        "        self.matched_V = set()\n",
        "\n",
        "    def greedy_match(self):\n",
        "        for u, v in self.edges:\n",
        "            # Check if both u and v are not already matched\n",
        "            if u not in self.matched_U and v not in self.matched_V:\n",
        "                # Add the edge to the matching\n",
        "                self.matching.add((u, v))\n",
        "                # Mark both vertices as matched\n",
        "                self.matched_U.add(u)\n",
        "                self.matched_V.add(v)\n",
        "\n",
        "        return self.matching\n",
        "\n",
        "# Define sets U and V\n",
        "U = {1, 2, 3, 4}  # Set U\n",
        "V = {'a', 'b', 'c', 'd'}  # Set V\n",
        "\n",
        "# List of edges between U and V\n",
        "edges = [(1, 'a'), (2, 'b'), (3, 'a'), (4, 'c'), (2, 'd')]\n",
        "\n",
        "# Create BipartiteMatcher object and find maximal matching\n",
        "matcher = BipartiteMatcher(U, V, edges)\n",
        "matching = matcher.greedy_match()\n",
        "\n",
        "print(\"Maximal Matching:\", matching)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW-4NpjnGJNi",
        "outputId": "c823512c-dbff-4ae0-d0f8-30f67b1cd5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximal Matching: {(4, 'c'), (1, 'a'), (2, 'b')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "class FlajoletMartin:\n",
        "    def __init__(self):\n",
        "        self.max_zero_bits = 0\n",
        "\n",
        "    def hash_element(self, element):\n",
        "        # Hash the element using MD5\n",
        "        hashed_value = int(hashlib.md5(element.encode('utf-8')).hexdigest(), 16)\n",
        "        return hashed_value\n",
        "\n",
        "    def count_trailing_zeros(self, hashed_value):\n",
        "        # Count trailing zeros in the binary representation of the hash\n",
        "        return bin(hashed_value).rstrip('0').count('0')\n",
        "\n",
        "    def add(self, element):\n",
        "        # Hash the element and count the trailing zeros\n",
        "        hashed_value = self.hash_element(element)\n",
        "        trailing_zeros = self.count_trailing_zeros(hashed_value)\n",
        "        # Update the maximum trailing zeros found\n",
        "        self.max_zero_bits = max(self.max_zero_bits, trailing_zeros)\n",
        "\n",
        "    def estimate(self):\n",
        "        # Estimate the number of distinct elements\n",
        "        return 2 ** self.max_zero_bits\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "fm = FlajoletMartin()\n",
        "data_stream = ['apple', 'banana', 'apple', 'orange', 'banana', 'grape', 'apple']\n",
        "\n",
        "for element in data_stream:\n",
        "    fm.add(element)\n",
        "\n",
        "print(\"Estimated number of distinct elements:\", fm.estimate())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7G5LBkZoyLQ",
        "outputId": "d00e6eb4-040d-46c0-b3df-351f8bc5420c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated number of distinct elements: 4722366482869645213696\n"
          ]
        }
      ]
    }
  ]
}